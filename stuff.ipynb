{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load public dataset from University of Tokyo\n",
    "!wget http://ss-takashi.sakura.ne.jp/corpus/jsut_ver1.1.zip\n",
    "!unzip jsut_ver1.1.zip\n",
    "\n",
    "path = 'jsut_ver1.1/basic5000/'\n",
    "df = pd.read_csv(path + 'transcript_utf8.txt', header = None, delimiter = \":\", names=[\"path\", \"sentence\"], index_col=False)\n",
    "df[\"path\"] = df[\"path\"].map(lambda x: path + 'wav/' + x + \".wav\")\n",
    "df.head()\n",
    "\n",
    "jsut_voice_train = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import training dataset\n",
    "common_voice_train = load_dataset('common_voice', 'ja',split='train+validation')\n",
    "common_voice_test = load_dataset('common_voice', 'ja', split='test')\n",
    "\n",
    "# Remove unwanted columns\n",
    "common_voice_train = common_voice_train.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"segment\", \"up_votes\"])\n",
    "common_voice_test = common_voice_test.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"segment\", \"up_votes\"])\n",
    "\n",
    "# Concat common voice and public dataset\n",
    "common_voice_train = datasets.concatenate_datasets([jsut_voice_train, common_voice_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wakati = MeCab.Tagger(\"-Owakati\")\n",
    "neo = neologdn.normalize\n",
    "\n",
    "# Unwanted token\n",
    "chars_to_ignore_regex = '[\\,\\、\\。\\．\\「\\」\\…\\？\\・\\!\\-\\;\\:\\\"\\“\\%\\‘\\”\\�]'\n",
    "\n",
    "def remove_special_characters(batch):\n",
    "    batch[\"sentence\"] = neologdn.normalize=(batch[\"sentence\"]).strip()\n",
    "    batch[\"sentence\"] = re.sub(chars_to_ignore_regex,'', batch[\"sentence\"]).strip()\n",
    "    return batch\n",
    "\n",
    "common_voice_train = common_voice_train.map(remove_special_characters)\n",
    "common_voice_test = common_voice_test.map(remove_special_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make metric function\n",
    "wer_metric = load_metric(\"wer\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_logits = pred.predictions\n",
    "    pred_ids = np.argmax(pred_logits, axis=-1)\n",
    "\n",
    "    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)\n",
    "\n",
    "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polynomial_decay_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps, num_training_steps, lr_end=1e-7, power=1.2, last_epoch=-1\n",
    "):\n",
    "\n",
    "    lr_init = optimizer.defaults[\"lr\"]\n",
    "    assert lr_init > lr_end, f\"lr_end ({lr_end}) must be be smaller than initial lr ({lr_init})\"\n",
    "\n",
    "    def lr_lambda(current_step: int):\n",
    "        if current_step < num_warmup_steps:\n",
    "            return float(current_step) / float(max(1, num_warmup_steps))\n",
    "        elif current_step > num_training_steps:\n",
    "            return lr_end / lr_init  # as LambdaLR multiplies by lr_init\n",
    "        else:\n",
    "            lr_range = lr_init - lr_end\n",
    "            decay_steps = num_training_steps - num_warmup_steps\n",
    "            pct_remaining = 1 - (current_step - num_warmup_steps) / decay_steps\n",
    "            decay = lr_range * pct_remaining ** power + lr_end\n",
    "            return decay / lr_init  # as LambdaLR multiplies by lr_init\n",
    "\n",
    "    return LambdaLR(optimizer, lr_lambda, last_epoch)\n",
    "\n",
    "# wrap custom learning scheduler with trainer\n",
    "class PolyTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def create_scheduler(self, num_training_steps: int):\n",
    "        self.lr_scheduler = get_polynomial_decay_schedule_with_warmup(self.optimizer,\n",
    "                                                                      num_warmup_steps=self.args.warmup_steps,\n",
    "                                                                      num_training_steps=num_training_steps)\n",
    "    def create_optimizer_and_scheduler(self, num_training_steps: int):\n",
    "        self.create_optimizer()\n",
    "        self.create_scheduler(num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "import MeCab\n",
    "import pykakasi\n",
    "import re\n",
    "\n",
    "#config\n",
    "wakati = MeCab.Tagger(\"-Owakati\")\n",
    "chars_to_ignore_regex = '[\\,\\、\\。\\．\\「\\」\\…\\？\\・]'\n",
    "\n",
    "#load model\n",
    "processor = Wav2Vec2Processor.from_pretrained(save_dir)\n",
    "test_model = Wav2Vec2ForCTC.from_pretrained(save_dir)\n",
    "test_model.to(\"cuda\")\n",
    "resampler = torchaudio.transforms.Resample(48_000, 16_000)\n",
    "\n",
    "#load testdata\n",
    "test_dataset = load_dataset(\"common_voice\", \"ja\", split=\"test\")\n",
    "wer = load_metric(\"wer\")\n",
    "\n",
    "# Preprocessing the datasets.\n",
    "def speech_file_to_array_fn(batch):\n",
    "    batch[\"sentence\"] = wakati.parse(batch[\"sentence\"]).strip()\n",
    "    batch[\"sentence\"] = re.sub(chars_to_ignore_regex,'', batch[\"sentence\"]).strip()\n",
    "    speech_array, sampling_rate = torchaudio.load(batch[\"path\"])\n",
    "    batch[\"speech\"] = resampler(speech_array).squeeze().numpy()\n",
    "    return batch\n",
    "\n",
    "test_dataset = test_dataset.map(speech_file_to_array_fn)\n",
    "\n",
    "# Preprocessing the datasets.\n",
    "# We need to read the aduio files as arrays\n",
    "def evaluate(batch):\n",
    "    inputs = processor(batch[\"speech\"], sampling_rate=16_000, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = test_model(inputs.input_values.to(\"cuda\"), attention_mask=inputs.attention_mask.to(\"cuda\")).logits\n",
    "    pred_ids = torch.argmax(logits, dim=-1)\n",
    "    batch[\"pred_strings\"] = processor.batch_decode(pred_ids)\n",
    "    return batch\n",
    "\n",
    "result = test_dataset.map(evaluate, batched=True, batch_size=8)\n",
    "\n",
    "print(\"WER: {:2f}\".format(100 * wer.compute(predictions=result[\"pred_strings\"], references=result[\"sentence\"])))\n",
    "\n",
    "# print some reusults\n",
    "pick = random.randint(0, len(common_voice_test_transcription)-1)\n",
    "input_dict = processor(common_voice_test[\"input_values\"][pick], return_tensors=\"pt\", padding=True)\n",
    "logits = test_model(input_dict.input_values.to(\"cuda\")).logits\n",
    "pred_ids = torch.argmax(logits, dim=-1)[0]\n",
    "\n",
    "print(\"Prediction:\")\n",
    "print(processor.decode(pred_ids).strip())\n",
    "\n",
    "print(\"\\nLabel:\")\n",
    "print(processor.decode(common_voice_test['labels'][pick]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
