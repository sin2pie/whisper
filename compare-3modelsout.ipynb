{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "#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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "cache_dir=\"/hf\"\n",
    "audio = \"URVRSP-209-A.mp3\"\n",
    "#task = \"automatic-speech-recognition\"\n",
    "model_1 = \"./merge_folder/whisper-medium-step_1/checkpoint_4000_merged/Merged_model\"\n",
    "#model_2 = \"./merge_folder/whisper-medium-step_1/checkpoint_4000_merged/Merged_model\"\n",
    "#model_3 = \"./merge_folder/whisper-medium-step_1/checkpoint_4000_merged/Merged_model\"\n",
    "# use_fast = True\n",
    "#generate_kwargs={\"language\": \"en\", \"task\":\"transcribe\"}#, \"return_timestamps\":\"true\"}\n",
    "# binary_output = True\n",
    "# chunk_length_s=10 \n",
    "# stride_length_s=(2, 2)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_1 = pipeline(task, model_1, device=device, torch_dtype=torch_dtype, generate_kwargs=generate_kwargs) #\n",
    "pipe_2 = pipeline(task, model_2, device=device, torch_dtype=torch_dtype, chunk_length_s=10, stride_length_s=(2, 2), generate_kwargs=generate_kwargs)\n",
    "pipe_3 = pipeline(task, model_3, device=device, torch_dtype=torch_dtype, generate_kwargs=generate_kwargs)\n",
    "b_1 = pipe_1(audio)\n",
    "b_2 = pipe_2(audio)\n",
    "b_3 = pipe_3(audio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b_1[\"text\"])\n",
    "#print(b_2[\"text\"]) \n",
    "#print(b_3[\"text\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepl\n",
    "\n",
    "auth_key = \"\" \n",
    "translator = deepl.Translator(auth_key)\n",
    "\n",
    "result1 = translator.translate_text((result[\"text\"]), target_lang=\"EN-US\")\n",
    "result2 = translator.translate_text((b_2[\"text\"]), target_lang=\"EN-US\")\n",
    "result3 = translator.translate_text((b_3[\"text\"]), target_lang=\"EN-US\")\n",
    "print(result1.text)\n",
    "print(result2.text) \n",
    "print(result3.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "\n",
    "cache_dir=\"/hf\"\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch_dtype = torch.bfloat16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "generate_kwargs={\"language\": \"ja\"}#, \"task\":\"translate\"}#, \"return_timestamps\":\"true\"}\n",
    "\n",
    "model_id = \"openai/whisper-medium\"#./merge_folder/whisper-medium-step_1/checkpoint_4000_merged/Merged_model\" #'distil-whisper/distil-large-v2'\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch_dtype,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_safetensors=True,\n",
    "    cache_dir=cache_dir,\n",
    "    #attn_implementation='flash_attention_2',\n",
    "    ).to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id, cache_dir=cache_dir)\n",
    "\n",
    "pipe = pipeline(\n",
    "    'automatic-speech-recognition',\n",
    "    model=model,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    max_new_tokens=128,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device=device,\n",
    "    generate_kwargs=generate_kwargs,\n",
    ")\n",
    "\n",
    "#dataset = load_dataset('lj_speech', 'main', split='train', cache_dir='data')\n",
    "#sample = dataset[0]['audio']\n",
    "sample= \"URVRSP-209-A.mp3\"\n",
    "\n",
    "start_time = time.time()\n",
    "result = pipe(sample)\n",
    "print('--- %s seconds ---' % (time.time() - start_time))\n",
    "print(result['text'].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import hashlib\n",
    "from pytube import YouTube\n",
    "from datetime import timedelta\n",
    "import os\n",
    "\n",
    "def download_video(url):\n",
    "    print(\"Start downloading\", url)\n",
    "    yt = YouTube(url)\n",
    "\n",
    "    hash_file = hashlib.md5()\n",
    "    hash_file.update(yt.title.encode())\n",
    "\n",
    "    file_name = f'{hash_file.hexdigest()}.mp4'\n",
    "\n",
    "    yt.streams.first().download(\"\", file_name)\n",
    "    print(\"Downloaded to\", file_name)\n",
    "\n",
    "    return {\n",
    "        \"file_name\": file_name,\n",
    "        \"title\": yt.title\n",
    "    }\n",
    "\n",
    "def transcribe_audio(path):\n",
    "    model = whisper.load_model(\"./sin2piusc/whisper-med_15k\") # Change this to your desired model\n",
    "    print(\"Whisper model loaded.\")\n",
    "    video = download_video(path)\n",
    "    transcribe = model.transcribe(video[\"file_name\"])\n",
    "    os.remove(video[\"file_name\"])\n",
    "    segments = transcribe['segments']\n",
    "\n",
    "    for segment in segments:\n",
    "        startTime = str(0)+str(timedelta(seconds=int(segment['start'])))+',000'\n",
    "        endTime = str(0)+str(timedelta(seconds=int(segment['end'])))+',000'\n",
    "        text = segment['text']\n",
    "        segmentId = segment['id']+1\n",
    "        segment = f\"{segmentId}\\n{startTime} --> {endTime}\\n{text[1:] if text[0] == ' ' else text}\\n\\n\"\n",
    "\n",
    "        srtFilename = os.path.join(r\"./\", \"your_srt_file_name.srt\")\n",
    "        with open(srtFilename, 'a', encoding='utf-8') as srtFile:\n",
    "            srtFile.write(segment)\n",
    "\n",
    "    return srtFilename\n",
    "\n",
    "link = \"https://www.youtube.com/watch?v=8qpwqDnvTok\"\n",
    "result = transcribe_audio(link)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
