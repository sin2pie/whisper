{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Iterable, Optional\n",
    "# from .decoding import decode as decode_function\n",
    "# from .decoding import detect_language as detect_language_function\n",
    "# from .transcribe import transcribe as transcribe_function\n",
    "\n",
    "import base64\n",
    "import gzip\n",
    "\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    WhisperForConditionalGeneration,\n",
    "    WhisperProcessor,\n",
    "    Seq2SeqTrainer,\n",
    "    TrainerCallback,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    TrainerState,\n",
    "    TrainerControl,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig,\n",
    "    WhisperTokenizer,\n",
    "    WhisperFeatureExtractor,\n",
    "    PushToHubCallback,\n",
    "    AutoTokenizer,\n",
    "    WhisperConfig,\n",
    "    AutoFeatureExtractor,\n",
    "    AutoProcessor,\n",
    "    AutoModel,\n",
    ")\n",
    "device = 'cuda' #if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LayerNorm(nn.LayerNorm):\n",
    "    def forward(self, x):\n",
    "        return super().forward(x.float()).type(x.dtype)\n",
    "\n",
    "class Linear(nn.Linear):\n",
    "    def forward(self, x):\n",
    "        return F.linear(x, self.weight.to(x.dtype),None if self.bias is None else self.bias.to(x.dtype),\n",
    "        )\n",
    "\n",
    "class Conv1d(nn.Conv1d): #https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n",
    "    def _conv_forward(self, x, weight, bias):\n",
    "        return super()._conv_forward(x, weight.to(x.dtype), None if bias is None else bias.to(x.dtype))\n",
    "\n",
    "def sinusoids(length, channels, max_timescale=10000):\n",
    "    assert channels % 2 == 0\n",
    "    log_timescale_increment = np.log(max_timescale) / (channels // 2 - 1)\n",
    "    inv_timescales = torch.exp(-log_timescale_increment * torch.arange(channels // 2))\n",
    "    scaled_time = torch.arange(length)[:, np.newaxis] * inv_timescales[np.newaxis, :]\n",
    "    return torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "    def forward(self, x):\n",
    "        out = self.embed(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, config = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.config = config\n",
    "        \n",
    "        self.q = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.v = nn.Linear(embed_dim, embed_dim)\n",
    "        self.o = nn.Linear(embed_dim, embed_dim)\n",
    "        self.scaling = self.head_dim**-0.5\n",
    "    \n",
    "    def _shape(self, tensor, seq_len, bsz):\n",
    "        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):  # Apply linear transformations and split heads           \n",
    "        Q = self.split_heads(self.k(Q))\n",
    "        K = self.split_heads(self.v(K))\n",
    "        V = self.split_heads(self.q(V))\n",
    "        output = self.scaled_dot_product_attention(Q, K, V, mask)   # Perform scaled dot-product attention  \n",
    "        attn_output = self.o(self.combine_heads(output))  # Combine heads and apply output transformation\n",
    "        return attn_output\n",
    "   \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        \n",
    "        attn_weights = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)  # Calculate attention scores\n",
    "        if mask is not None: # Apply mask if provided (useful for preventing attention to certain parts like padding)\n",
    "            attn_weights = attn_weights.masked_fill(mask == 0, -1e9)      \n",
    "        probs = torch.softmax(attn_weights, dim=-1)  # Softmax is applied to obtain attention probabilities\n",
    "        attn_probs = torch.matmul(probs, V) # Multiply by values to obtain the final output\n",
    "        return attn_probs\n",
    "        \n",
    "    def split_heads(self, x):  # Reshape the input to have num_heads for multi-head attention\n",
    "        batch_size, seq_length, self.embed_dim = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):  # Combine the multiple heads back to original shape\n",
    "        batch_size, _, seq_length, self.head_dim = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.embed_dim)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention_2(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.q = Linear(embed_dim, embed_dim)\n",
    "        self.k = Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.v = Linear(embed_dim, embed_dim)\n",
    "        self.o = Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x, xa = None, mask = None, kv_cache = None):\n",
    "        Q = self.q(x)\n",
    "        if kv_cache is None or xa is None or self.key not in kv_cache:\n",
    "            K = self.k(x if xa is None else xa)\n",
    "            V = self.v(x if xa is None else xa)\n",
    "        else:\n",
    "            K = kv_cache[self.k]\n",
    "            V = kv_cache[self.v]\n",
    "        wv, qk = self.qkv_attention(Q, K, V, mask)\n",
    "        return self.out(wv), qk\n",
    "\n",
    "    def qkv_attention(self, q, k, v, mask= None):\n",
    "        n_batch, n_ctx, embed_dim = q.shape\n",
    "        scale = (embed_dim // self.num_heads) ** -0.25\n",
    "        q = q.view(*q.shape[:2], self.num_heads, -1).permute(0, 2, 1, 3) * scale\n",
    "        k = k.view(*k.shape[:2], self.num_heads, -1).permute(0, 2, 3, 1) * scale\n",
    "        v = v.view(*v.shape[:2], self.num_heads, -1).permute(0, 2, 1, 3)\n",
    "        qk = q @ k\n",
    "        if mask is not None:\n",
    "            qk = qk + mask[:n_ctx, :n_ctx]\n",
    "        qk = qk.float()\n",
    "        w = F.softmax(qk, dim=-1).to(q.dtype)\n",
    "        return (w @ v).permute(0, 2, 1, 3).flatten(start_dim=2), qk.detach()\n",
    "\n",
    "class ResidualAttentionBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, cross_attention = False):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.attn_ln = LayerNorm(embed_dim)\n",
    "\n",
    "        self.cross_attn = (MultiHeadAttention(embed_dim, num_heads) if cross_attention else None)\n",
    "        self.cross_attn_ln = LayerNorm(embed_dim) if cross_attention else None\n",
    "\n",
    "        n_mlp = embed_dim * 4\n",
    "        self.mlp = nn.Sequential(Linear(embed_dim, n_mlp), nn.GELU(), Linear(n_mlp, embed_dim))\n",
    "        self.mlp_ln = LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(\n",
    "        self, x, xa = None, mask = None, kv_cache = None):\n",
    "        \n",
    "        x = x + self.attn(self.attn_ln(x), mask=mask, kv_cache=kv_cache)[0]\n",
    "        if self.cross_attn:\n",
    "            x = x + self.cross_attn(self.cross_attn_ln(x), xa, kv_cache=kv_cache)[0]\n",
    "        x = x + self.mlp(self.mlp_ln(x))\n",
    "        return x\n",
    "    \n",
    "class AudioEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, n_mels, n_ctx, embed_dim, num_heads, n_layer):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.conv1 = Conv1d(n_mels, embed_dim, kernel_size=3, padding=1)\n",
    "        self.conv2 = Conv1d(embed_dim, embed_dim, kernel_size=3, stride=2, padding=1)\n",
    "        self.register_buffer(\"positional_embedding\", sinusoids(n_ctx, embed_dim))\n",
    "\n",
    "        self.blocks: Iterable[ResidualAttentionBlock] = nn.ModuleList(\n",
    "            [ResidualAttentionBlock(embed_dim, num_heads) for _ in range(n_layer)]\n",
    "        )\n",
    "        self.ln_post = LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = F.gelu(self.conv1(x))\n",
    "        x = F.gelu(self.conv2(x))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = (x + self.positional_embedding).to(x.dtype)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.ln_post(x)\n",
    "        return x\n",
    "    \n",
    "class TextDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, n_vocab, n_ctx, embed_dim, num_heads, n_layer):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_embedding = nn.Embedding(n_vocab, embed_dim)\n",
    "        self.positional_embedding = nn.Parameter(torch.empty(n_ctx, embed_dim))\n",
    "\n",
    "        self.blocks: Iterable[ResidualAttentionBlock] = nn.ModuleList(\n",
    "            [\n",
    "                ResidualAttentionBlock(embed_dim, num_heads, cross_attention=True)\n",
    "                for _ in range(n_layer)\n",
    "            ])\n",
    "        self.ln = LayerNorm(embed_dim)\n",
    "\n",
    "        mask = torch.empty(n_ctx, n_ctx).fill_(-np.inf).triu_(1)\n",
    "        self.register_buffer(\"mask\", mask, persistent=False)\n",
    "\n",
    "    def forward(self, x, xa, kv_cache = None):\n",
    "        offset = next(iter(kv_cache.values())).shape[1] if kv_cache else 0\n",
    "        x = (self.token_embedding(x) + self.positional_embedding[offset : offset + x.shape[-1]])\n",
    "        x = x.to(xa.dtype)\n",
    "        for block in self.blocks:\n",
    "            x = block(x, xa, mask=self.mask, kv_cache=kv_cache)\n",
    "        x = self.ln(x)\n",
    "        logits = (x @ torch.transpose(self.token_embedding.weight.to(x.dtype), 0, 1)).float()\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, embed_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_seq_length, embed_dim)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * -(math.log(10000.0) / embed_dim))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(embed_dim, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(embed_dim, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dims, src_vocab_size, tgt_vocab_size, embed_dim, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.dims = dims\n",
    "        self.encoder = AudioEncoder(\n",
    "            self.dims.n_mels,\n",
    "            self.dims.n_audio_ctx,\n",
    "            self.dims.n_audio_state,\n",
    "            self.dims.n_audio_head,\n",
    "            self.dims.n_audio_layer,\n",
    "        )\n",
    "        self.decoder = TextDecoder(\n",
    "            self.dims.n_vocab,\n",
    "            self.dims.n_text_ctx,\n",
    "            self.dims.n_text_state,\n",
    "            self.dims.n_text_head,\n",
    "            self.dims.n_text_layer,\n",
    "        )\n",
    "    \n",
    "        all_heads = torch.zeros(\n",
    "            self.dims.n_text_layer, self.dims.n_text_head, dtype=torch.bool\n",
    "        )\n",
    "        all_heads[self.dims.n_text_layer // 2 :] = True\n",
    "        self.register_buffer(\"alignment_heads\", all_heads.to_sparse(), persistent=False)\n",
    "        \n",
    "        \n",
    "        all_heads = torch.zeros(self.dims.n_text_layer, self.dims.n_text_head, dtype=torch.bool)\n",
    "        all_heads[self.dims.n_text_layer // 2 :] = True\n",
    "        self.register_buffer(\"alignment_heads\", all_heads.to_sparse(), persistent=False)\n",
    "\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, embed_dim)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, embed_dim)\n",
    "        self.positional_encoding = PositionalEncoding(embed_dim, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(embed_dim, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(embed_dim, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(embed_dim, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def set_alignment_heads(self, dump):\n",
    "        array = np.frombuffer(gzip.decompress(base64.b85decode(dump)), dtype=bool).copy()\n",
    "        mask = torch.from_numpy(array).reshape(self.dims.n_text_layer, self.dims.n_text_head)\n",
    "        self.register_buffer(\"alignment_heads\", mask.to_sparse(), persistent=False)\n",
    "\n",
    "    def embed_audio(self, mel):\n",
    "        return self.encoder(mel)\n",
    "\n",
    "    def logits(self, tokens, audio_features):\n",
    "        return self.decoder(tokens, audio_features)\n",
    "\n",
    "    def forward(self, mel, tokens):\n",
    "        return self.decoder(tokens, self.encoder(mel))\n",
    "    \n",
    "        @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    @property\n",
    "    def is_multilingual(self):\n",
    "        return self.dims.n_vocab >= 51865\n",
    "\n",
    "    @property\n",
    "    def num_languages(self):\n",
    "        return self.dims.n_vocab - 51765 - int(self.is_multilingual)\n",
    "\n",
    "    def install_kv_cache_hooks(self, cache: Optional[dict] = None):\n",
    "\n",
    "        cache = {**cache} if cache is not None else {}\n",
    "        hooks = []\n",
    "\n",
    "        def save_to_cache(module, _, output):\n",
    "            if module not in cache or output.shape[1] > self.dims.n_text_ctx:\n",
    "                cache[module] = output\n",
    "            else:\n",
    "                cache[module] = torch.cat([cache[module], output], dim=1).detach()\n",
    "            return cache[module]\n",
    "\n",
    "        def install_hooks(layer: nn.Module):\n",
    "            if isinstance(layer, MultiHeadAttention):\n",
    "                hooks.append(layer.key.register_forward_hook(save_to_cache))\n",
    "                hooks.append(layer.value.register_forward_hook(save_to_cache))\n",
    "\n",
    "        self.decoder.apply(install_hooks)\n",
    "        return cache, hooks\n",
    "\n",
    "    detect_language = detect_language_function\n",
    "    transcribe = transcribe_function\n",
    "    decode = decode_function\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = 5000\n",
    "tgt_vocab_size = 6000\n",
    "embed_dim = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "d_ff = 2048\n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "\n",
    "transformer = Transformer(src_vocab_size, tgt_vocab_size, embed_dim, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
    "\n",
    "# Generate random sample data\n",
    "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.train()\n",
    "torch.save(transformer.state_dict(), \"my_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(4):\n",
    "    optimizer.zero_grad()\n",
    "    output = transformer(src_data, tgt_data[:, :-1])\n",
    "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.eval()\n",
    "\n",
    "# Generate random sample validation data\n",
    "val_src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "val_tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    val_output = transformer(val_src_data, val_tgt_data[:, :-1])\n",
    "    val_loss = criterion(val_output.contiguous().view(-1, tgt_vocab_size), val_tgt_data[:, 1:].contiguous().view(-1))\n",
    "    print(f\"Validation Loss: {val_loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @dataclass\n",
    "# class ModelDimensions:\n",
    "#     n_mels: int\n",
    "#     n_audio_ctx: int\n",
    "#     n_audio_state: int\n",
    "#     n_audio_head: int\n",
    "#     n_audio_layer: int\n",
    "#     n_vocab: int\n",
    "#     n_text_ctx: int\n",
    "#     n_text_state: int\n",
    "#     n_text_head: int\n",
    "#     n_text_layer: int\n",
    "\n",
    "# trainer = Seq2SeqTrainer(\n",
    "#     args=training_args,\n",
    "#     model=transformer,\n",
    "#     train_dataset=vectorized_dataset, # [\"train\"],\n",
    "#     eval_dataset=vectorized_dataset_test, # [\"test\"],\n",
    "#     data_collator=data_collator,\n",
    "#     tokenizer=processor.feature_extractor,\n",
    "#     callbacks=[SavePeftModelCallback(),ShuffleCallback()],\n",
    "#     compute_metrics=compute_metrics, \n",
    "#     )\n",
    "\n",
    "# from dataclasses import dataclass\n",
    "# from typing import List, Optional, Any, Dict, List, Union\n",
    "# model_name_or_path = \"./1\"\n",
    "# language = \"japanese\"\n",
    "# task = \"transcribe\"\n",
    "\n",
    "# def prepare_dataset(batch):\n",
    "#     audio = batch[\"audio\"]\n",
    "#     batch[\"input_features\"] = processor.feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "#     batch[\"audio_length\"] = len(audio[\"array\"]) / audio[\"sampling_rate\"]\n",
    "#     batch[\"labels\"] = processor.tokenizer(batch[\"sentence\"]).input_ids\n",
    "#     return batch\n",
    "\n",
    "# @dataclass\n",
    "# class DataCollatorSpeechSeq2SeqWithPadding:\n",
    "#     processor: Any\n",
    "\n",
    "#     def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "#         input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
    "#         batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
    "#         label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
    "#         labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
    "#         labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "#         if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
    "#             labels = labels[:, 1:]\n",
    "#         batch[\"labels\"] = labels\n",
    "#         return batch\n",
    "    \n",
    "# feature_extractor = WhisperFeatureExtractor.from_pretrained(\n",
    "#     model_name_or_path,\n",
    "#     do_normalize = False,\n",
    "#     device=\"cuda\",\n",
    "#     sampling_rate=16000,\n",
    "#     return_attention_mask=True,\n",
    "#     truncation=True,\n",
    "#     n_fft=1024,\n",
    "#     n_mels=512,\n",
    "#     hop_length=320,\n",
    "#     pad_mode=\"reflect\",\n",
    "#     power=2.0,\n",
    "#     norm=\"slaney\",\n",
    "#     mel_scale=\"htk\",\n",
    "#     )\n",
    "# tokenizer = WhisperTokenizer.from_pretrained(\n",
    "#     model_name_or_path,\n",
    "#     language=language,\n",
    "#     task=task,\n",
    "#     )\n",
    "# processor = WhisperProcessor.from_pretrained(\n",
    "#     model_name_or_path,\n",
    "#     tokenizer=tokenizer,\n",
    "#     feature_extractor=feature_extractor,\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultiheadAttention(nn.Module):\n",
    "#     def __init__(self, dmodel, dk, dv, num_heads):\n",
    "#         super().__init__()\n",
    "#         self.num_heads = num_heads\n",
    "#         self.dmodel = dmodel\n",
    "\n",
    "#         self.proj_q, self.bias_q = self._get_proj_bias(dk)\n",
    "#         self.proj_k, self.bias_k = self._get_proj_bias(dk)\n",
    "#         self.proj_v, self.bias_v = self._get_proj_bias(dv)\n",
    "        \n",
    "#         self.output_proj = nn.Linear(dv * num_heads, dmodel, bias=False)\n",
    "\n",
    "#         self.register_buffer('scale', torch.tensor(dk, dtype=float).sqrt())\n",
    "    \n",
    "#     def _get_proj_bias(self, hidsize):\n",
    "#         proj = nn.Parameter(torch.Tensor(self.num_heads, self.dmodel, hidsize))\n",
    "#         bias = nn.Parameter(torch.Tensor(1, self.num_heads, 1, hidsize))\n",
    "#         nn.init.xavier_uniform_(proj)\n",
    "#         nn.init.constant_(bias, 0.)\n",
    "#         return proj, bias\n",
    "\n",
    "#     def forward(self, q, k, v):\n",
    "#         # batch, seqlen, dmodel\n",
    "#         q = (q.unsqueeze(1) @ self.proj_q) + self.bias_q\n",
    "#         k = (k.unsqueeze(1) @ self.proj_k) + self.bias_k\n",
    "#         v = (v.unsqueeze(1) @ self.proj_v) + self.bias_v\n",
    "#         # batch, head, seqlen, dk|dv\n",
    "\n",
    "#         q, k, v = q.unsqueeze(3), k.unsqueeze(2), v.unsqueeze(2)\n",
    "#         # q: (batch, head, qlen, 1, dk)\n",
    "#         # k, v: (batch, head, 1, kvlen, dk|dv)\n",
    "#         logits = (q * k / self.scale).sum(-1, keepdim=True)\n",
    "#         # batch, head, qlen, kvlen, 1\n",
    "#         weighted_v = F.softmax(logits, -2) * v\n",
    "#         # batch, head, qlen, kvlen, dv\n",
    "#         heads = weighted_v.sum(-2)\n",
    "#         # batch, head, qlen, dv\n",
    "#         hid = torch.cat(heads.unbind(1), -1)\n",
    "#         # batch, qlen, dv * head\n",
    "#         output = self.output_proj(hid)\n",
    "#         # batch, qlen, dmodel\n",
    "#         return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
